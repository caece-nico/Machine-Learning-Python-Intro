{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## En este ejemplo solo vamos a separar un sataset en training y test set y a encodear los features y escalar las varable\n",
    "## numericas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sea\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['symboling', 'make', 'fuel_type', 'aspiration', 'num_of_doors',\n",
      "       'body_style', 'drive_wheels', 'engine_location', 'wheel_base', 'length',\n",
      "       'width', 'height', 'curb_weight', 'engine_type', 'num_of_cylinders',\n",
      "       'engine_size', 'fuel_system', 'bore', 'stroke', 'compression_ratio',\n",
      "       'horsepower', 'peak_rpm', 'city_mpg', 'highway_mpg', 'price',\n",
      "       'log_price'],\n",
      "      dtype='object')\n",
      "0    13495\n",
      "1    16500\n",
      "2    16500\n",
      "3    13950\n",
      "4    17450\n",
      "5    15250\n",
      "6    17710\n",
      "7    18920\n",
      "8    23875\n",
      "9    16430\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "datos = pd.read_csv('Auto_Data_Preped.csv')\n",
    "print(datos.columns)\n",
    "print(datos['price'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 1\n",
    "## Hacemos el preprocessing de las variables para convertirlas a numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hardtop_convert' 'hatchback' 'sedan' 'wagon']\n",
      "[0 0 1 2 2 2 2 3 2 2 2 2 2 2 2 2 2 1 1 2 1 1 1 1 2 2 2 3 1 1 1 1 1 1 2 3 1\n",
      " 1 2 2 2 2 2 1 2 2 2 1 1 1 2 2 1 2 1 2 2 1 2 2 2 3 0 2 2 0 2 0 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 3 2 1 2 3 0 1 2 2 3 2 1 1 1 2 2 3 3 2 2 3 3 2 2 2\n",
      " 1 1 1 2 2 3 1 1 0 0 0 1 2 1 2 1 2 1 1 1 2 2 2 2 2 3 3 3 3 1 1 1 3 3 3 2 1\n",
      " 2 1 2 1 2 2 1 2 1 0 0 1 0 1 0 2 2 1 2 1 1 1 2 3 2 2 2 2 2 2 2 0 1 2 2 3 2\n",
      " 3 2 3 2 3 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(datos['body_style'].unique())\n",
    "Features = datos['body_style']\n",
    "enc = preprocessing.LabelEncoder()\n",
    "enc.fit(Features)\n",
    "Features = enc.transform(Features)\n",
    "print(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paso 2\n",
    "## A scikit learn no le asi, debe ser un vector de arrays con (0,1), por o tanto debe estar encodeado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = preprocessing.OneHotEncoder()\n",
    "encoded = ohe.fit(Features.reshape(-1,1))\n",
    "Features = encoded.transform(Features.reshape(-1,1)).toarray()\n",
    "Features[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 3\n",
    "## como ya creamos un encoder y tenemos el array Feature inicializado. vamos a hacer lo mismo desntro de una funcion con un loop\n",
    "## para las restantes variables categoricas que son de nuestro interes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 14)\n",
      "[[1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "columnas = ['fuel_type', 'aspiration', 'drive_wheels', 'num_of_cylinders']\n",
    "\n",
    "def crea_encoders(datos, columna):\n",
    "    ##creamos los valores numericos\n",
    "    features_aux = datos[columna]\n",
    "    enc = preprocessing.LabelEncoder()\n",
    "    enc.fit(features_aux)\n",
    "    features_aux = enc.transform(features_aux)\n",
    "    ## encreamos el array de encoders\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    encoded = ohe.fit(features_aux.reshape(-1,1))\n",
    "    return encoded.transform(features_aux.reshape(-1,1)).toarray()\n",
    "\n",
    "for col in columnas:\n",
    "    auxiliar = crea_encoders(datos,col)\n",
    "    Features = np.concatenate([Features,auxiliar], axis = 1)\n",
    "    \n",
    "print(Features.shape)\n",
    "print(Features[:6,:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 5\n",
    "## agregamos las columnas numericas al array Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.concatenate([Features,np.array(datos[['curb_weight', 'horsepower', 'city_mpg']])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  2.548e+03 1.110e+02 2.100e+01]\n",
      " [1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  2.548e+03 1.110e+02 2.100e+01]\n",
      " [0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  2.823e+03 1.540e+02 1.900e+01]\n",
      " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  2.337e+03 1.020e+02 2.400e+01]\n",
      " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  2.824e+03 1.150e+02 1.800e+01]\n",
      " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  2.507e+03 1.100e+02 1.900e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(Features[:6,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 6.\n",
    "## Con las variables numerics en el mismo Array procedemos a hacer el split de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(9988)\n",
    "labels = np.array(datos['log_price'])\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx,test_size = 40)\n",
    "x_train = Features[indx[0],:]\n",
    "y_train = np.ravel(labels[indx[0]])\n",
    "x_test = Features[indx[1],:]\n",
    "y_test = np.ravel(labels[indx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00e+00 0.00e+00 1.00e+00 0.00e+00 0.00e+00 1.00e+00 1.00e+00 0.00e+00\n",
      " 0.00e+00 1.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e+00 2.41e+03 8.40e+01\n",
      " 2.60e+01]\n",
      "[9.9751103  8.87150535]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[3,:])\n",
    "print(y_test[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASO 7 \n",
    "## Ya tenemos el split hecho, solo queda scalar las variables numericas en el array tanto x_train como x_test.\n",
    "## en y_train y test no porque el log_prces lo habiamos creado mientras haciamos FEATURE_ING en el tutorial anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 17)\n",
      "[[-0.5384069  -1.26225437  1.33602998]\n",
      " [ 0.96837381  1.51064566 -1.00126852]]\n"
     ]
    }
   ],
   "source": [
    "## hacemos el scaller de la variable 14 en adelante porque son las numericas.\n",
    "scaler = preprocessing.StandardScaler().fit(x_train[:,14:])\n",
    "x_train[:,14:] = scaler.transform(x_train[:,14:])\n",
    "x_test[:,14:] = scaler.transform(x_test[:,14:])\n",
    "print(x_train.shape)\n",
    "print(x_train[:2,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## con todo esto ya tenemos el modelo para empezar a trabajar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
